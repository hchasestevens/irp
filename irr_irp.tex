\documentclass[a4paper,11pt]{proposal}
\usepackage{times}  %% This makes the body base font times...
\usepackage{url}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{amsmath, amsthm}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage[olditem]{paralist}
\usepackage{epsfig}
\usepackage{qtree}


\setlength{\headheight}{14pt} %Fixes headheight warning
\widowpenalty=1000
\clubpenalty=1000



% Put in your details here
\title{Applying Statistical Language Modeling to Genetic Programming}
\crest{\includegraphics[width=40mm]{crest.pdf}}
\author{H. Chase Stevens}
\studentnumber{s1107496}
% For IRR
\collegeordept{School of Informatics}
% For IRP
\collegeordept{Supervisor: Dr. I. Stark}


\university{University of Edinburgh}

% For IRR
%\degree{Informatics Research Review}
% For IRP
\degree{M.Sc. Project Proposal}


\degreedate{April 2016} 



\begin{document}
\maketitle 
\parindent=0mm


\pagenumbering{roman}
\setcounter{tocdepth}{2}

\clearpage

\newpage

\pagenumbering{arabic}

\setlength{\parskip}{1ex} 


\section{Introduction} \label{sec:intro}

\section{Background and related work} \label{sec:back}

\subsection{Application of language modeling to formal languages}

\subsection{Genetic programming}

\section{Methodology} \label{sec:disc}

\subsection{Compilation of Python corpus}
The creation of a language model for Python will require a large and representative corpus of Python source code. While previous work has employed small collections of hand-chosen open-source Python projects for use as a corpus \cite{tu2014} \cite{caliskan2015}, and other work has resulted in the compilation of large corpora for languages such as Java \cite{allamanis2013}, there does not appear to be a suitably large Python source code corpus publicly available for use in this project at present; ergo, the first objective of this project will be to compile one. To do so, source code will be downloaded from open-source projects hosted on the popular code repository host GitHub. 

Ideally, the Python corpus compiled for this project will consist of idiomatic, useful, and well-written Python source code; for this project, good-quality code is of special importance, as inexecutable code may still be syntactically valid and parsable. In compiling a similar corpus for the Java programming language, Allamanis and Sutton \cite{allamanis2013} sought to maintain a minimum level of quality in the GitHub repositories used by filtering out those that had not been forked at least once; in this project, for which I am planning to create a smaller corpus, repositories will instead be used if they exceed some minimum number of stars or forks. To find repositories matching these criteria, the GitHub search API will be used, which allows filtering on not only number of stars and forks but also on project language.

\subsection{Creation of language model from Python corpus}

While much work has been done on modeling programming languages through the use of n-gram models (e.g. \cite{hindle2012}, \cite{allamanis2013}), genetic programming is most naturally applied to tree-like structures as opposed to sequences, and, as such, the language model constructed for this project will be a probabilistic grammar over Python's Abstract Syntax Tree (AST).

When considered as sequences of tokens, programming languages are purported to contain less information per token than natural languages \cite{hindle2012}. For example, under a trigram model, Java source code has an estimated average per-token cross-entropy of 4.9 bits \cite{allamanis2013}, compared to an estimated cross-entropy value of nearly 8 bits per word in English \cite{brown1992}. However, despite this,  an individual node in an AST can provide relatively little semantic information or information about non-children descendant nodes within the tree, easily allowing for the generation of unrunnable programs. Consider the example presented in \autoref{fig:1}, from which it can be clearly seen that a Call node in Python's AST contains no information about e.g. the types of the arguments with which the specified function is to be called. In order to combat this problem and encourage the generation of valid Python code, I plan to annotate AST nodes with variables learned through expectation maximization (EM), an unsupervised technique which has been shown to reduce perplexity measurements of probabilistic grammars over natural languages without the need for careful manual feature selection \cite{matsuzaki2005} \cite{petrov2006}.

Since EM as an optimization technique is not guaranteed to converge on a global optimum and relies on randomized initializations which have demonstrable effects on the quality of the final grammar obtained \cite{matsuzaki2005}, I plan to generate several candidate probabilistic grammars using EM, as well as probabilistic grammar without annotations for comparison. Considering that cursory initial investigation suggests that several thousand Python projects on GitHub will meet the proposed criteria for inclusion in the aforementioned source code corpus, generating these language models might prove to be quite computationally intensive. Ideally, I would like to make use of a Hadoop cluster to distribute and parallelize the creation of each language model, thereby reducing overall time spent in model creation. Failing this, an alternative would be to sample a small subset of the corpus for use in training the language models, however, as corpus size has been shown to greatly impact language model efficacy \citep{allamanis2013}, I would be reticent to pursue this course of action.

\begin{figure}
\qtreecenterfalse
\hskip 0.5in \Tree [.Call [.(func) [.Name ``\texttt{map}" ] ] [.(args) [.Name ``\texttt{len}" ] [.List [.Str ``\texttt{a}" ] [.Str ``\texttt{bc}" ]]]]
\hskip 0.25in \Tree [.Call [.(func) [.Name ``\texttt{map}" ] ] [.(args) [.Name ``\texttt{len}" ] [.List [.Num \texttt{1} ] [.Num \texttt{2} ]]]]

\caption{Comparison of the respective Python AST representations of the semantically valid expression ``\texttt{map(len, ['a', 'bc'])}" (left) and the invalid expression ``\texttt{map(len, [1, 2])}" (right). Note that across both cases the types of the direct descendant nodes of ``Call" (i.e. Name, Name, and List) are identical.}
\label{fig:1}
\end{figure}


\subsection{Application of language model to genetic programming system}

The use of probabilistic grammars in genetic programming has been well established, with many variations having been presented in the literature \cite{shan2006}. The key difference between previous work and the system proposed here is that, while previous approaches have assumed initial uniform probability distributions over competing production rules which are then updated in accordance with the relative success of the solutions produced incorporating each production \cite{whigham1995} \cite{ratle2001} \cite{keber2002}, my system will use the aforementioned Python language model to supply initial probabilities, which will similarly be updated as appropriate to meritorious solutions identified for specific tasks. 

The direction of genetic programming's search strategy is not alone sufficient to ensure solutions are found in reasonable time: a phenomenon known as ``bloat", in which solutions incorporate increasingly large portions of code which do not contribute to their fitness to the task at hand, can have serious deleterious effects on the run-time of the search, as each individual within the genetic programming population becomes more expensive to evaluate. To combat this, I plan to use the parsimony techniques introduced in Poli \cite{poli2003} and Poli \& McPhee \cite{poli2008}, which introduce pressures against excessive solution growth both during the fitness evaluation and individual selection phases of genetic programming.

Another important consideration when allowing for the generation of arbitrary code is the safety with which such code can be run; code may exhibit side-effects that harm or render inoperable the underlying system on which it is executed. While not a perfect solution, \texttt{pysandbox}\footnote{https://pypi.python.org/pypi/pysandbox/} offers a reasonable level of protection against undesirable side-effects provided the code has not been maliciously written so as to intentionally subvert the sandbox's constraints. 


\section{Evaluation}

In the literature, many genetic programming approaches are evaluated using grammars tailored to particular domains \cite{mcdermott2012}. However, in this project, by necessity, the genetic programming system to be developed will be operating on the general grammar of the Python programming language. Therefore, a suitable task will be one in which solutions lend themselves to representation and manipulation as Python ASTs.

While, often, genetic programming is used to generate code from scratch, recent work has successfully applied genetic programming to the automated repair of pre-existing programs \cite{weimer2009}. As in this case manipulation is done on the program's AST, this task would be an ideal evaluative measure for the proposed genetic programming system. Although prior work in this domain has been evaluated on snapshots of open source software during instances in which the software failed to pass a suite of automated tests, given the scope of this project, evaluation will instead either be performed on test-compliant software which has had bugs deliberately introduced through random AST mutation, or on hand-written examples, dependent on the feasibility of the former given the project's time constraints.

The primary means of evaluating the proposed genetic programming system will be demonstrating how many (if any) software repair tasks it is able to solve. Time permitting, the system's performance, both in terms of number of solutions found and speed with which solutions are identified, will be compared against a baseline genetic programming system which does not incorporate an initial language model as induced using the Python corpus. A small-scale qualitative analysis of the naturalness or idiomaticity of code produced by both systems would also be possible.


\section{Timeline}

\section{Conclusions}

\newpage
\bibliographystyle{IEEEtran}
\bibliography{irr_irp}
\end{document}
